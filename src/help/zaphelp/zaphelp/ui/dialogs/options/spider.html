<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<title>Options Spider screen</title>
</head>
<body bgcolor="#ffffff">
	<h1>Options Spider screen</h1>
	<p>
		This screen allows you to configure the <a
			href="../../../start/concepts/spider.html">spider</a> options:
	</p>

	<h3>Maximum depth to crawl</h3>
	The parameter defines the maximum depth in the crawling process where a
	page must be found in order for it to be processed. Resources found
	deeper than this level are not fetched and parsed by the spider.
	<p>
		The depth is calculated starting from the seeds, so, if a Spider scan
		starts with only a single URL (eg. Spider URL), the depth is
		calculated from this one. However, if the scan starts with multiple
		seeds (eg. Spider site), a resource is processed if it's depth
		relative to <i>any</i> of the seeds is less than the defined one.
	</p>

	<h3>Number of threads used</h3>
	The spider is multi-threaded and this is the number that defines the
	maximum number of worker threads used in the crawling process.

	<h3>Domain Pattern</h3>
	The normal behavior of the spider is to only follow links to resources
	found on the same domain as the page where the scan started. However,
	this option allows you to define additional domains that are considered
	"in scope" during the crawling process. Pages on these domains are
	processed during the scan. Domain suffixes have to be separated by ';'.
	The '*' can also be used to mark 'any sequence of characters'. Other
	regex special characters ( \[^)(?$| ) are automatically being
	eliminated from the pattern and should not be used.
	<p>For example, if the domain pattern is '*example.com', both
		foo.example.com and example.com will match. If the domain pattern is
		just example.com, only it will match. If the domain is *.example.com,
		foo.example.com will match, but example.com will not match.</p>

	<h3>Process forms</h3>
	During the crawling process, the behaviour of the spider when it
	encounters HTML forms is defined by this option. If disabled, the HTML
	forms will not be processed at all. If enabled, the HTML forms with the
	method defined as HTTP GET will be submitted with some generated
	values. The behaviour when encountering forms with the method defined
	as HTTP POST is configured by the next option.

	<h3>POST forms</h3>
	As briefly described in the previous paragraph (Process Forms), this
	option configures the behaviour of the spider when
	<i>Process Forms</i> is enabled and when encountering HTML forms that
	have to be POSTed.

	<h3>Use Cookies</h3>
	This option defines whether the spider should send back the cookies
	received in server responses with subsequent requests. If disabled, all
	cookie related header fields will be ignored.

	<h3>Parse HTML Comments</h3>
	This option defines whether the spider should also spider the HTML
	comments searching for links to resources. Only the resources found in
	commented valid HTML tags will be processed.

	<h3>Parse 'robots.txt' files</h3>
	This option defines whether the spider should also spider the
	robots.txt files found on websites, searching for links to resources.
	This option does not define whether the spider should follow the rules
	imposed by the robots.txt file.


	<h2>See also</h2>
	<table>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../overview.html">UI Overview</a></td>
			<td>for an overview of the user interface</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../start/concepts/spider.html">Spider</a></td>
			<td>for an overview of the Spider</td>
		</tr>
		<tr>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td><a href="../../../ui/tabs/spider.html">Spider Tab</a></td>
			<td>for an overview of the Spider tab</td>
		</tr>
	</table>

</body>
</html>
